2023-11-07 11:10:08.711953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
done reading training data
done reading training data
done reading training data
done reading training data
[1, 1, 1, 0, 1, 1, 1, 1, 1, 1]
loaded datasets
okk
start training
  0%|          | 0/285 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/285 [00:00<02:53,  1.64it/s]  1%|          | 2/285 [00:00<01:40,  2.81it/s]  1%|          | 3/285 [00:00<01:17,  3.62it/s]  1%|â–         | 4/285 [00:01<01:06,  4.21it/s]  2%|â–         | 5/285 [00:01<01:00,  4.62it/s]  2%|â–         | 6/285 [00:01<00:56,  4.91it/s]  2%|â–         | 7/285 [00:01<00:54,  5.11it/s]  3%|â–Ž         | 8/285 [00:01<00:52,  5.25it/s]  3%|â–Ž         | 9/285 [00:02<00:51,  5.35it/s]  4%|â–Ž         | 10/285 [00:02<00:50,  5.41it/s]  4%|â–         | 11/285 [00:02<00:50,  5.45it/s]  4%|â–         | 12/285 [00:02<00:49,  5.48it/s]  5%|â–         | 13/285 [00:02<00:49,  5.50it/s]  5%|â–         | 14/285 [00:02<00:49,  5.52it/s]  5%|â–Œ         | 15/285 [00:03<00:48,  5.53it/s]  6%|â–Œ         | 16/285 [00:03<00:48,  5.54it/s]  6%|â–Œ         | 17/285 [00:03<00:48,  5.55it/s]  6%|â–‹         | 18/285 [00:03<00:48,  5.55it/s]  7%|â–‹         | 19/285 [00:03<00:47,  5.56it/s]  7%|â–‹         | 20/285 [00:04<00:47,  5.56it/s]  7%|â–‹         | 21/285 [00:04<00:47,  5.57it/s]  8%|â–Š         | 22/285 [00:04<00:47,  5.57it/s]  8%|â–Š         | 23/285 [00:04<00:47,  5.57it/s]  8%|â–Š         | 24/285 [00:04<00:46,  5.58it/s]  9%|â–‰         | 25/285 [00:04<00:46,  5.57it/s]  9%|â–‰         | 26/285 [00:05<00:46,  5.57it/s]  9%|â–‰         | 27/285 [00:05<00:46,  5.57it/s] 10%|â–‰         | 28/285 [00:05<00:46,  5.56it/s] 10%|â–ˆ         | 29/285 [00:05<00:46,  5.56it/s] 11%|â–ˆ         | 30/285 [00:05<00:45,  5.56it/s] 11%|â–ˆ         | 31/285 [00:05<00:45,  5.56it/s] 11%|â–ˆ         | 32/285 [00:06<00:45,  5.55it/s] 12%|â–ˆâ–        | 33/285 [00:06<00:45,  5.55it/s] 12%|â–ˆâ–        | 34/285 [00:06<00:45,  5.56it/s] 12%|â–ˆâ–        | 35/285 [00:06<00:45,  5.55it/s] 13%|â–ˆâ–Ž        | 36/285 [00:06<00:44,  5.54it/s] 13%|â–ˆâ–Ž        | 37/285 [00:07<00:44,  5.54it/s] 13%|â–ˆâ–Ž        | 38/285 [00:07<00:44,  5.55it/s] 14%|â–ˆâ–Ž        | 39/285 [00:07<00:44,  5.55it/s] 14%|â–ˆâ–        | 40/285 [00:07<00:44,  5.55it/s] 14%|â–ˆâ–        | 41/285 [00:07<00:44,  5.54it/s] 15%|â–ˆâ–        | 42/285 [00:07<00:43,  5.54it/s] 15%|â–ˆâ–Œ        | 43/285 [00:08<00:43,  5.54it/s] 15%|â–ˆâ–Œ        | 44/285 [00:08<00:43,  5.54it/s] 16%|â–ˆâ–Œ        | 45/285 [00:08<00:43,  5.54it/s] 16%|â–ˆâ–Œ        | 46/285 [00:08<00:43,  5.54it/s] 16%|â–ˆâ–‹        | 47/285 [00:08<00:42,  5.55it/s] 17%|â–ˆâ–‹        | 48/285 [00:09<00:42,  5.55it/s] 17%|â–ˆâ–‹        | 49/285 [00:09<00:42,  5.55it/s] 18%|â–ˆâ–Š        | 50/285 [00:09<00:42,  5.55it/s] 18%|â–ˆâ–Š        | 51/285 [00:09<00:42,  5.54it/s] 18%|â–ˆâ–Š        | 52/285 [00:09<00:42,  5.54it/s] 19%|â–ˆâ–Š        | 53/285 [00:09<00:41,  5.53it/s] 19%|â–ˆâ–‰        | 54/285 [00:10<00:41,  5.53it/s] 19%|â–ˆâ–‰        | 55/285 [00:10<00:41,  5.53it/s] 20%|â–ˆâ–‰        | 56/285 [00:10<00:41,  5.53it/s]
  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|â–ˆâ–        | 3/26 [00:00<00:01, 21.76it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 6/26 [00:00<00:01, 16.84it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 8/26 [00:00<00:01, 15.93it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 10/26 [00:00<00:01, 15.44it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 12/26 [00:00<00:00, 15.14it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/26 [00:00<00:00, 14.94it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/26 [00:01<00:00, 14.82it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 18/26 [00:01<00:00, 14.73it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 20/26 [00:01<00:00, 14.67it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/26 [00:01<00:00, 14.63it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 24/26 [00:01<00:00, 14.61it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:01<00:00, 15.74it/s][A                                                
                                               [A 20%|â–ˆâ–ˆ        | 57/285 [00:12<00:41,  5.53it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:01<00:00, 15.74it/s][A
                                               [A{'eval_loss': 0.5234820246696472, 'eval_accuracy': 0.7671568627450981, 'eval_runtime': 1.8022, 'eval_samples_per_second': 452.791, 'eval_steps_per_second': 14.427, 'epoch': 1.0}
Traceback (most recent call last):
  File "/cluster/home/shimin/small_LL_claim_classification/train_new.py", line 159, in <module>
    trainer.train()
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 1664, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 2034, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 2308, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 2434, in _save_checkpoint
    self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer_callback.py", line 98, in save_to_json
    with open(json_path, "w", encoding="utf-8") as f:
OSError: [Errno 122] Disk quota exceeded
 20%|â–ˆâ–ˆ        | 57/285 [00:14<00:57,  3.95it/s]
