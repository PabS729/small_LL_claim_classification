2023-11-07 11:10:08.711953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
done reading training data
done reading training data
done reading training data
done reading training data
[1, 1, 1, 0, 1, 1, 1, 1, 1, 1]
loaded datasets
okk
start training
  0%|          | 0/285 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/285 [00:00<02:53,  1.64it/s]  1%|          | 2/285 [00:00<01:40,  2.81it/s]  1%|          | 3/285 [00:00<01:17,  3.62it/s]  1%|▏         | 4/285 [00:01<01:06,  4.21it/s]  2%|▏         | 5/285 [00:01<01:00,  4.62it/s]  2%|▏         | 6/285 [00:01<00:56,  4.91it/s]  2%|▏         | 7/285 [00:01<00:54,  5.11it/s]  3%|▎         | 8/285 [00:01<00:52,  5.25it/s]  3%|▎         | 9/285 [00:02<00:51,  5.35it/s]  4%|▎         | 10/285 [00:02<00:50,  5.41it/s]  4%|▍         | 11/285 [00:02<00:50,  5.45it/s]  4%|▍         | 12/285 [00:02<00:49,  5.48it/s]  5%|▍         | 13/285 [00:02<00:49,  5.50it/s]  5%|▍         | 14/285 [00:02<00:49,  5.52it/s]  5%|▌         | 15/285 [00:03<00:48,  5.53it/s]  6%|▌         | 16/285 [00:03<00:48,  5.54it/s]  6%|▌         | 17/285 [00:03<00:48,  5.55it/s]  6%|▋         | 18/285 [00:03<00:48,  5.55it/s]  7%|▋         | 19/285 [00:03<00:47,  5.56it/s]  7%|▋         | 20/285 [00:04<00:47,  5.56it/s]  7%|▋         | 21/285 [00:04<00:47,  5.57it/s]  8%|▊         | 22/285 [00:04<00:47,  5.57it/s]  8%|▊         | 23/285 [00:04<00:47,  5.57it/s]  8%|▊         | 24/285 [00:04<00:46,  5.58it/s]  9%|▉         | 25/285 [00:04<00:46,  5.57it/s]  9%|▉         | 26/285 [00:05<00:46,  5.57it/s]  9%|▉         | 27/285 [00:05<00:46,  5.57it/s] 10%|▉         | 28/285 [00:05<00:46,  5.56it/s] 10%|█         | 29/285 [00:05<00:46,  5.56it/s] 11%|█         | 30/285 [00:05<00:45,  5.56it/s] 11%|█         | 31/285 [00:05<00:45,  5.56it/s] 11%|█         | 32/285 [00:06<00:45,  5.55it/s] 12%|█▏        | 33/285 [00:06<00:45,  5.55it/s] 12%|█▏        | 34/285 [00:06<00:45,  5.56it/s] 12%|█▏        | 35/285 [00:06<00:45,  5.55it/s] 13%|█▎        | 36/285 [00:06<00:44,  5.54it/s] 13%|█▎        | 37/285 [00:07<00:44,  5.54it/s] 13%|█▎        | 38/285 [00:07<00:44,  5.55it/s] 14%|█▎        | 39/285 [00:07<00:44,  5.55it/s] 14%|█▍        | 40/285 [00:07<00:44,  5.55it/s] 14%|█▍        | 41/285 [00:07<00:44,  5.54it/s] 15%|█▍        | 42/285 [00:07<00:43,  5.54it/s] 15%|█▌        | 43/285 [00:08<00:43,  5.54it/s] 15%|█▌        | 44/285 [00:08<00:43,  5.54it/s] 16%|█▌        | 45/285 [00:08<00:43,  5.54it/s] 16%|█▌        | 46/285 [00:08<00:43,  5.54it/s] 16%|█▋        | 47/285 [00:08<00:42,  5.55it/s] 17%|█▋        | 48/285 [00:09<00:42,  5.55it/s] 17%|█▋        | 49/285 [00:09<00:42,  5.55it/s] 18%|█▊        | 50/285 [00:09<00:42,  5.55it/s] 18%|█▊        | 51/285 [00:09<00:42,  5.54it/s] 18%|█▊        | 52/285 [00:09<00:42,  5.54it/s] 19%|█▊        | 53/285 [00:09<00:41,  5.53it/s] 19%|█▉        | 54/285 [00:10<00:41,  5.53it/s] 19%|█▉        | 55/285 [00:10<00:41,  5.53it/s] 20%|█▉        | 56/285 [00:10<00:41,  5.53it/s]
  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|█▏        | 3/26 [00:00<00:01, 21.76it/s][A
 23%|██▎       | 6/26 [00:00<00:01, 16.84it/s][A
 31%|███       | 8/26 [00:00<00:01, 15.93it/s][A
 38%|███▊      | 10/26 [00:00<00:01, 15.44it/s][A
 46%|████▌     | 12/26 [00:00<00:00, 15.14it/s][A
 54%|█████▍    | 14/26 [00:00<00:00, 14.94it/s][A
 62%|██████▏   | 16/26 [00:01<00:00, 14.82it/s][A
 69%|██████▉   | 18/26 [00:01<00:00, 14.73it/s][A
 77%|███████▋  | 20/26 [00:01<00:00, 14.67it/s][A
 85%|████████▍ | 22/26 [00:01<00:00, 14.63it/s][A
 92%|█████████▏| 24/26 [00:01<00:00, 14.61it/s][A
100%|██████████| 26/26 [00:01<00:00, 15.74it/s][A                                                
                                               [A 20%|██        | 57/285 [00:12<00:41,  5.53it/s]
100%|██████████| 26/26 [00:01<00:00, 15.74it/s][A
                                               [A{'eval_loss': 0.5234820246696472, 'eval_accuracy': 0.7671568627450981, 'eval_runtime': 1.8022, 'eval_samples_per_second': 452.791, 'eval_steps_per_second': 14.427, 'epoch': 1.0}
Traceback (most recent call last):
  File "/cluster/home/shimin/small_LL_claim_classification/train_new.py", line 159, in <module>
    trainer.train()
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 1664, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 2034, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 2308, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer.py", line 2434, in _save_checkpoint
    self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))
  File "/cluster/apps/nss/gcc-8.2.0/python/3.11.2/x86_64/lib64/python3.11/site-packages/transformers/trainer_callback.py", line 98, in save_to_json
    with open(json_path, "w", encoding="utf-8") as f:
OSError: [Errno 122] Disk quota exceeded
 20%|██        | 57/285 [00:14<00:57,  3.95it/s]
